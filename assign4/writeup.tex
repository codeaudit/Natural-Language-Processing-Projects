
\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}

\title{CS 288: Reranking}

\author{Anting Shen \\
  23566738 \\
  {\tt antingshen@berkeley.edu} \\
}
\date{October 17, 2014}

\begin{document}
\maketitle

\section{Introduction}


\section{Learning Algorithms}

use best F1


\section{Features}

Using best-list position and rule indicators as simple features, the reranker has 45,991 features,
trains in 110 seconds, and achieves a F1 of 85.03.

I then added the 1-width binned log probability assigned to a tree as an indicator feature.
While Charniak and Johnson used the probability as a feature directly,
Johnson and Ural's used both real probability as well as binned log probability indicators,
and the binned indicators had a greater effect on F1. This may be due to binning probabilities
allows the reranker to accurately learn nonlinear relationships between log probability and tree quality.
This feature increased F1 to 85.21. Adding lengths per rule further increased it to 85.24.

Many potential features involve some sort of lexicalization, which comes with the issue of sparsity
where words aren't seen frequently in training data. To combat this, I borrow the idea from the
Less Grammar, More Features paper of representing words as their longest suffix that occurs more
than 100 times.\footnote{The word "SEAQ" presented itself as an interesting edge case, and was represented
as empty string}

Using this new technique, I added features to mark the starting and ending words of rules.
Case sensitivity was tested, and it made no difference so case insensitive was chosen for performance.
Adding these features brought feature count to 143,529. It trains in 153 seconds
and achieves a F1 of 85.63, which is now more than 1.5 points above the 1-best baseline.





\end{document}
