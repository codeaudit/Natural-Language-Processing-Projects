
\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}

\title{CS 288: Reranking}

\author{Anting Shen \\
  23566738 \\
  {\tt antingshen@berkeley.edu} \\
}
\date{October 17, 2014}

\begin{document}
\maketitle

\section{Introduction}


\section{Learning Algorithms}

use best F1


\section{Features}

Using best-list position and rule indicators as simple features, the reranker has 45,991 features,
trains in 110 seconds, and achieves a F1 of 85.03.

I then added the 1-width binned log probability assigned to a tree as an indicator feature.
While Charniak and Johnson used the probability as a feature directly,
Johnson and Ural's used both real probability as well as binned log probability indicators,
and the binned indicators had a greater effect on F1. This may be due to binning probabilities
allows the reranker to accurately learn nonlinear relationships between log probability and tree quality.
This feature increased F1 to 85.21. Adding lengths per rule further increased it to 85.24.

Many potential features involve some sort of lexicalization, which comes with the issue of sparsity
where words aren't seen frequently in training data. To combat this, I borrow the idea from the
Less Grammar, More Features paper of representing words as their

Training took 152894 millis
1-best baseline
 [Average]  P: 84.6 R: 83.65 F1: 84.12 EX: 22.94
Decoding took 59 millis
Your system
 [Average]  P: 86.07 R: 85.2 F1: 85.63 EX: 28.83
Decoding took 652 millis
 150,497





\end{document}
